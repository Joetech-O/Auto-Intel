{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cab982f-e3c1-460e-9d47-0cc5cd4e6b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/opt/anaconda3/lib/python3.12/site-packages/gensim/models/ldamodel.py:847: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  perwordbound = self.bound(chunk, subsample_ratio=subsample_ratio) / (subsample_ratio * corpus_words)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Uploaded\n"
     ]
    }
   ],
   "source": [
    "# news_analysis_pipeline_refactored.py\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# -------------------- Setup --------------------\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# --- DB (uncomment ONE) ---\n",
    "#engine = create_engine('postgresql://auto_intel:auto-intel@localhost/auto-intel')\n",
    "DB_URL = \"postgresql://auto_intel_user:G2ifL76ULDi9YOFJ4tHZ1ikUEORMx7Oe@dpg-d2i7ptvdiees73d1b4lg-a.oregon-postgres.render.com/auto_intel\"\n",
    "engine = create_engine(DB_URL, pool_pre_ping=True)\n",
    "\n",
    "# -------------------- Load ---------------------\n",
    "df = pd.read_sql(\"SELECT * FROM car_news;\", engine)\n",
    "df['content'] = df['content'].astype(str)\n",
    "\n",
    "# -------------------- Sentiment (VADER) --------------------\n",
    "# IMPORTANT: run on raw content (not token list) to keep negation and punctuation\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_score(text: str) -> float:\n",
    "    return analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "def vader_label(score: float) -> str:\n",
    "    if score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "df['sentiment_score'] = df['content'].apply(vader_score)\n",
    "df['sentiment_label'] = df['sentiment_score'].apply(vader_label)\n",
    "\n",
    "# Persist lightweight sentiment snapshot (extend columns if you need)\n",
    "df[['id','publication_date','sentiment_score','sentiment_label']].to_sql(\n",
    "    'temp_sentiments', con=engine, if_exists='replace', index=False\n",
    ")\n",
    "\n",
    "# -------------------- Topic Modelling (LDA) --------------------\n",
    "# Domain stopwords: remove car-generic terms, KEEP sentiment words like \"good\", \"bad\", \"love\", etc.\n",
    "domain_sw = {\n",
    "    'car','cars','vehicle','vehicles','drive','driving','review','reviews',\n",
    "    'news','range','brand','model','models','ev','electric','petrol','diesel'\n",
    "}\n",
    "base_sw = set(stopwords.words('english'))\n",
    "topic_stopwords = base_sw | domain_sw\n",
    "\n",
    "def tokenize_for_topics(text: str):\n",
    "    # Keep only alphabetic tokens for LDA; lowercased\n",
    "    toks = word_tokenize(text.lower())\n",
    "    return [w for w in toks if w.isalpha() and w not in topic_stopwords and len(w) > 2]\n",
    "\n",
    "df['tokens'] = df['content'].apply(tokenize_for_topics)\n",
    "\n",
    "# Build dictionary/corpus\n",
    "dictionary = corpora.Dictionary(df['tokens'])\n",
    "# Optional pruning to reduce noise\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=5000)\n",
    "\n",
    "corpus = [dictionary.doc2bow(toks) for toks in df['tokens']]\n",
    "\n",
    "lda_model = gensim.models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    num_topics=5,\n",
    "    id2word=dictionary,\n",
    "    passes=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "def dominant_topic(ldamodel, bow):\n",
    "    topics = ldamodel.get_document_topics(bow)\n",
    "    return max(topics, key=lambda x: x[1])[0] if topics else None\n",
    "\n",
    "df['dominant_topic'] = [dominant_topic(lda_model, bow) for bow in corpus]\n",
    "topic_keywords = {i: [w for w, _ in lda_model.show_topic(i, topn=10)] for i in range(lda_model.num_topics)}\n",
    "df['topic_keywords'] = df['dominant_topic'].map(topic_keywords)\n",
    "\n",
    "df[['id','title','dominant_topic','topic_keywords','sentiment_label','sentiment_score']].to_sql(\n",
    "    'news_articles_topics', con=engine, index=False, if_exists='replace'\n",
    ")\n",
    "print(\"Successfully Uploaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc9b03d7-36a2-409a-a91d-20d0c2faa85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Supervised Sentiment (TF-IDF + LR) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.000     0.000     0.000         4\n",
      "     neutral      1.000     1.000     1.000       254\n",
      "    positive      0.973     1.000     0.986       144\n",
      "\n",
      "    accuracy                          0.990       402\n",
      "   macro avg      0.658     0.667     0.662       402\n",
      "weighted avg      0.980     0.990     0.985       402\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  0   0   4]\n",
      " [  0 254   0]\n",
      " [  0   0 144]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Uses TF-IDF + Logistic Regression with class_weight='balanced' and n-grams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Use your existing sentiment_label from annotation if available.\n",
    "labels = df['sentiment_label']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['content'], labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        analyzer=\"word\",\n",
    "        ngram_range=(1,2),        \n",
    "        min_df=3,\n",
    "        max_df=0.8,\n",
    "        strip_accents=\"unicode\",\n",
    "        sublinear_tf=True\n",
    "    )),\n",
    "    (\"logreg\", LogisticRegression(\n",
    "        max_iter=300,\n",
    "        class_weight=\"balanced\",   \n",
    "        n_jobs=None,\n",
    "        C=2.0                      \n",
    "    ))\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Supervised Sentiment (TF-IDF + LR) ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa114ef2-a08a-475a-a9ed-4a4f1ae28516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dfc0286f-4613-42eb-be68-3b5d98b31952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ner_analysis_refactored.py\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Load stronger model (better accuracy for ORG/PRODUCT names)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_trf\")\n",
    "except:\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Connect DB\n",
    "#engine = create_engine('postgresql://auto_intel:auto-intel@localhost/auto-intel')\n",
    "DB_URL = \"postgresql://auto_intel_user:G2ifL76ULDi9YOFJ4tHZ1ikUEORMx7Oe@dpg-d2i7ptvdiees73d1b4lg-a.oregon-postgres.render.com/auto_intel\"\n",
    "engine = create_engine(DB_URL, pool_pre_ping=True)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_sql(\"SELECT * FROM car_news;\", engine)\n",
    "df['content'] = df['content'].astype(str)\n",
    "\n",
    "# --- Entity extraction ---\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    ents = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"ORG\",\"GPE\",\"PRODUCT\"}:\n",
    "            ents.append((ent.text.strip().lower(), ent.label_))\n",
    "    return ents\n",
    "\n",
    "df['named_entities'] = df['content'].apply(extract_entities)\n",
    "\n",
    "# Flatten\n",
    "all_ents = [ent for sublist in df['named_entities'] for ent in sublist]\n",
    "entity_df = pd.DataFrame(all_ents, columns=['entity','label'])\n",
    "\n",
    "# Count frequency\n",
    "entity_counts = entity_df.groupby(['label','entity']).size().reset_index(name='count')\n",
    "\n",
    "# Top N\n",
    "filtered = entity_counts.sort_values(by='count', ascending=False).head(100)\n",
    "\n",
    "# Save summary\n",
    "filtered.to_sql(\"named_entities_summary\", engine, index=False, if_exists='replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae8152-01b7-4db6-b170-dc34b8891d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f563b53-84de-425a-ab3f-9eee4a64a431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b0cbdf-f1ed-4deb-be27-94aa954a7ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a81a2ad1-77f1-4df8-88d2-d6580c2531f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded spaCy model: en_core_web_md\n",
      "Saved: car_news_named_entities, car_news_ner_proxy_metrics\n"
     ]
    }
   ],
   "source": [
    "# ner_extract_automotive.py\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# --- Load best available spaCy model ---\n",
    "for _m in (\"en_core_web_trf\", \"en_core_web_md\", \"en_core_web_sm\"):\n",
    "    try:\n",
    "        nlp = spacy.load(_m)\n",
    "        print(f\"Loaded spaCy model: {_m}\")\n",
    "        break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# --- DB ---\n",
    "#engine = create_engine(\"postgresql://auto_intel_user:G2ifL76ULDi9YOFJ4tHZ1ikUEORMx7Oe@dpg-d2i7ptvdiees73d1b4lg-a.oregon-postgres.render.com/auto_intel\")\n",
    "DB_URL = \"postgresql://auto_intel_user:G2ifL76ULDi9YOFJ4tHZ1ikUEORMx7Oe@dpg-d2i7ptvdiees73d1b4lg-a.oregon-postgres.render.com/auto_intel\"\n",
    "engine = create_engine(DB_URL, pool_pre_ping=True)\n",
    "df = pd.read_sql(\"SELECT id, title, content FROM car_news;\", engine)\n",
    "df[\"content\"] = df[\"content\"].astype(str).fillna(\"\")\n",
    "\n",
    "# --- Brands & helpful lists ---\n",
    "BRANDS = [\n",
    "    \"Porsche\",\"Bmw\",\"Mercedes\",\"mercedes-benz\",\"audi\",\"volkswagen\",\"vw\",\"toyota\",\"honda\",\"ford\",\n",
    "    \"chevrolet\",\"tesla\",\"byd\",\"renault\",\"peugeot\",\"citroen\",\"skoda\",\"seat\",\"ferrari\",\"lamborghini\",\n",
    "    \"aston martin\",\"mclaren\",\"alfa romeo\",\"fiat\",\"nissan\",\"hyundai\",\"kia\",\"mg\",\"geely\",\"volvo\",\n",
    "    \"polestar\",\"gwm\",\"ora\",\"cupra\",\"dacia\",\"opel\",\"vauxhall\",\"jeep\",\"ram\",\"dodge\",\"subaru\",\"suzuki\",\n",
    "    \"mazda\",\"lexus\",\"infiniti\",\"acura\",\"cadillac\",\"lincoln\",\"rolls-royce\",\"bentley\",\"bugatti\",\"lotus\",\n",
    "    \"ds\",\"mini\",\"smart\",\"saab\",\"chery\",\"nio\",\"xpeng\",\"lucid\",\"rivian\",\"tata\",\"mahindra\",\"proton\",\n",
    "    \"perodua\",\"holden\",\"daewoo\"\n",
    "]\n",
    "SHORT_WHITELIST = {\"Uk\",\"Us\",\"Eu\",\"vw\",\"mg\",\"byd\",\"bmw\"}\n",
    "\n",
    "# Common model patterns (token regex). We’ll label as PRODUCT.\n",
    "# Examples covered: 911, F-150, C-HR, ID.4, Model 3, Civic Type R, GR Yaris, i5, iX, GLC, EQS, M3, X5, etc.\n",
    "# PRODUCT_PATTERNS = [\n",
    "#     # Tesla\n",
    "#     {\"label\":\"PRODUCT\",\"pattern\":[{\"LOWER\":\"model\"},{\"TEXT\":{\"REGEX\":\"[S3XY]\"}}]},\n",
    "#     # Porsche 911 / 718 etc.\n",
    "#     {\"label\":\"PRODUCT\",\"pattern\":[{\"LOWER\":{\"IN\":BRANDS}}, {\"TEXT\":{\"REGEX\":\"\\\\d{2,3}\"}}]},\n",
    "#     # Ford F-150, F150, Ranger Raptor\n",
    "#     {\"label\":\"PRODUCT\",\"pattern\":[{\"LOWER\":{\"IN\":BRANDS}}, {\"TEXT\":{\"REGEX\":\"[A-Z]-?\\\\d{2,3}|[A-Za-z]+\"}}, {\"TEXT\":{\"REGEX\":\"raptor\"}}],},\n",
    "#     {\"label\":\"PRODUCT\",\"pattern\":[{\"LOWER\":{\"IN\":BRANDS}}, {\"TEXT\":{\"REGEX\":\"[A-Z]-?\\\\d{2,3}\"}}]},\n",
    "#     # VW ID.3 / ID.4 / ID Buzz\n",
    "#     {\"label\":\"PRODUCT\",\"pattern\":[{\"LOWER\":{\"IN\":BRANDS}}, {\"LOWER\":{\"REGEX\":\"id\\\\.?|buzz\"}}, {\"TEXT\":{\"REGEX\":\"\\\\d(\\\\.\\\\d)?\"},\"OP\":\"?\"}]},\n",
    "#     # Toyota GR Yaris / C-HR / Yaris Cross\n",
    "#     {\"label\":\"PRODUCT\",\"pattern\":[{\"LOWER\":{\"IN\":BRANDS}}, {\"LOWER\":{\"REGEX\":\"gr|c-hr|yaris|corolla|supra|prius\"}}, {\"LOWER\":{\"REGEX\":\"cross|sport|prime|gr\"},\"OP\":\"?\"}]},\n",
    "#     # BMW M3 / X5 / i4 / i5 / iX\n",
    "#     {\"label\":\"PRODUCT\",\"pattern\":[{\"LOWER\":{\"IN\":BRANDS}}, {\"TEXT\":{\"REGEX\":\"[MXi][A-Z]?$|[MXi]?\\\\d{1,3}\"}}]},\n",
    "#     # Merc GLC / EQS / EQE / C63\n",
    "#     {\"label\":\"PRODUCT\",\"pattern\":[{\"LOWER\":{\"IN\":BRANDS}}, {\"TEXT\":{\"REGEX\":\"[A-Z]{2,3}\\\\d{0,3}\"}}]},\n",
    "#     # Generic Brand + Model word/number with hyphens (C-HR, Z9 GT, Type R)\n",
    "#     {\"label\":\"PRODUCT\",\"pattern\":[{\"LOWER\":{\"IN\":BRANDS}}, {\"TEXT\":{\"REGEX\":\"[A-Za-z0-9-]{1,6}\"}}, {\"TEXT\":{\"REGEX\":\"gt|type|r|rs|gr\"},\"OP\":\"?\"}]},\n",
    "# ]\n",
    "\n",
    "# --- Add EntityRuler before NER so patterns can complement base model ---\n",
    "# Set overwrite_ents=False so we don't nuke good base spans; patterns fill gaps.\n",
    "if \"entity_ruler\" not in nlp.pipe_names:\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", config={\"overwrite_ents\": False})\n",
    "else:\n",
    "    ruler = nlp.get_pipe(\"entity_ruler\")\n",
    "\n",
    "# Add brand phrases as ORG (helps if base model misses them)\n",
    "brand_patterns = [{\"label\":\"ORG\", \"pattern\": b} for b in BRANDS]\n",
    "ruler.add_patterns(brand_patterns) #+ PRODUCT_PATTERNS)\n",
    "\n",
    "def normalize_entity(text):\n",
    "    t = text.strip()\n",
    "    low = t.lower()\n",
    "    if len(low) <= 2 and low not in SHORT_WHITELIST:\n",
    "        return \"\"  # drop tiny junk\n",
    "    return low\n",
    "\n",
    "KEEP = {\"ORG\",\"GPE\",\"PRODUCT\"}\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    out = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in KEEP:\n",
    "            norm = normalize_entity(ent.text)\n",
    "            if norm:\n",
    "                out.append((norm, ent.label_))\n",
    "    return out\n",
    "\n",
    "df[\"named_entities\"] = df[\"content\"].apply(extract_entities)\n",
    "all_ents = [e for ents in df[\"named_entities\"] for e in ents]\n",
    "entity_df = pd.DataFrame(all_ents, columns=[\"entity\",\"label\"])\n",
    "\n",
    "# --- Aggregate & persist ---\n",
    "entity_counts = entity_df.groupby([\"label\",\"entity\"]).size().reset_index(name=\"count\")\n",
    "top_entities = entity_counts.sort_values(\"count\", ascending=False).head(200)\n",
    "top_entities.to_sql(\"car_news_named_entities\", engine, index=False, if_exists=\"replace\")\n",
    "\n",
    "# # --- Quick proxy metrics to track improvement over time ---\n",
    "# proxy = entity_counts.groupby(\"label\").agg(\n",
    "#     mentions=(\"count\",\"sum\"),\n",
    "#     distinct_entities=(\"entity\",\"nunique\")\n",
    "# ).reset_index()\n",
    "# proxy.to_sql(\"car_news_ner_proxy_metrics\", engine, index=False, if_exists=\"replace\")\n",
    "\n",
    "print(\"Saved: car_news_named_entities, car_news_ner_proxy_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc21137-cc12-4b53-bacc-7c942973799f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b722f9a-2ba0-4a04-be97-23e91901c677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mentions per label:\n",
      "      label  mentions\n",
      "0      GPE      1565\n",
      "1      ORG      4125\n",
      "2   PERSON      1475\n",
      "3  PRODUCT      1153\n",
      "\n",
      "Distinct entities per label:\n",
      "      label  distinct_entities\n",
      "0      GPE                 22\n",
      "1      ORG                 90\n",
      "2   PERSON                 43\n",
      "3  PRODUCT                 45\n",
      "\n",
      "Entity length quantiles:\n",
      " count    200.000000\n",
      "mean       6.850000\n",
      "std        3.719769\n",
      "min        2.000000\n",
      "10%        3.000000\n",
      "25%        4.000000\n",
      "50%        6.000000\n",
      "75%        8.000000\n",
      "90%       11.000000\n",
      "95%       13.000000\n",
      "max       32.000000\n",
      "Name: len, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ner_proxy_checks.py\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#engine = create_engine(\"postgresql://auto_intel_user:G2ifL76ULDi9YOFJ4tHZ1ikUEORMx7Oe@dpg-d2i7ptvdiees73d1b4lg-a.oregon-postgres.render.com/auto_intel\")\n",
    "DB_URL = \"postgresql://auto_intel_user:G2ifL76ULDi9YOFJ4tHZ1ikUEORMx7Oe@dpg-d2i7ptvdiees73d1b4lg-a.oregon-postgres.render.com/auto_intel\"\n",
    "engine = create_engine(DB_URL, pool_pre_ping=True)\n",
    "\n",
    "ents = pd.read_sql(\"SELECT label, entity, count FROM car_news_named_entities;\", engine)\n",
    "\n",
    "# 1) Coverage per label\n",
    "cov = ents.groupby(\"label\")[\"count\"].sum().reset_index(name=\"mentions\")\n",
    "print(\"\\nMentions per label:\\n\", cov)\n",
    "\n",
    "# 2) Distinct entities per label\n",
    "distincts = ents.groupby(\"label\")[\"entity\"].nunique().reset_index(name=\"distinct_entities\")\n",
    "print(\"\\nDistinct entities per label:\\n\", distincts)\n",
    "\n",
    "# 3) Entity length distribution (helps spot junk)\n",
    "ents[\"len\"] = ents[\"entity\"].str.len()\n",
    "print(\"\\nEntity length quantiles:\\n\", ents[\"len\"].describe(percentiles=[.1,.25,.5,.75,.9,.95]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e53d76-802d-4188-90f1-5150ed597a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f7ed3c-02ae-4e0d-903a-d02c2384c7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6e0fdb2-5231-48b0-bf49-735fed05edd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded spaCy model: en_core_web_md\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# car_reviews_ner_refined.py\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# ---- Load best available model ----\n",
    "for _m in (\"en_core_web_trf\", \"en_core_web_md\", \"en_core_web_sm\"):\n",
    "    try:\n",
    "        nlp = spacy.load(_m)\n",
    "        print(f\"Loaded spaCy model: {_m}\")\n",
    "        break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ---- DB ----\n",
    "#engine = create_engine('postgresql://auto_intel:auto-intel@localhost/auto-intel')\n",
    "DB_URL = \"postgresql://auto_intel_user:G2ifL76ULDi9YOFJ4tHZ1ikUEORMx7Oe@dpg-d2i7ptvdiees73d1b4lg-a.oregon-postgres.render.com/auto_intel\"\n",
    "engine = create_engine(DB_URL, pool_pre_ping=True)\n",
    "\n",
    "# engine = create_engine(DB_URL)  # swap if using Render\n",
    "\n",
    "# ---- Data ----\n",
    "df = pd.read_sql(\"SELECT id, title, verdict FROM car_review;\", engine)\n",
    "df[\"title\"] = df[\"title\"].astype(str).fillna(\"\")\n",
    "df[\"verdict\"] = df[\"verdict\"].astype(str).fillna(\"\")\n",
    "# Use more context than 'verdict' alone:\n",
    "df[\"text\"] = (df[\"title\"].str.strip() + \". \" + df[\"verdict\"].str.strip()).str.strip()\n",
    "\n",
    "# ---- Automotive gazetteer/patterns ----\n",
    "BRANDS = [\n",
    "    \"porsche\",\"bmw\",\"mercedes\",\"mercedes-benz\",\"audi\",\"volkswagen\",\"vw\",\"toyota\",\"honda\",\"ford\",\n",
    "    \"chevrolet\",\"tesla\",\"byd\",\"renault\",\"peugeot\",\"citroen\",\"skoda\",\"seat\",\"ferrari\",\"lamborghini\",\n",
    "    \"aston martin\",\"mclaren\",\"alfa romeo\",\"fiat\",\"nissan\",\"hyundai\",\"kia\",\"mg\",\"geely\",\"volvo\",\n",
    "    \"polestar\",\"gwm\",\"ora\",\"cupra\",\"dacia\",\"opel\",\"vauxhall\",\"jeep\",\"ram\",\"dodge\",\"subaru\",\"suzuki\",\n",
    "    \"mazda\",\"lexus\",\"infiniti\",\"acura\",\"cadillac\",\"lincoln\",\"rolls-royce\",\"bentley\",\"bugatti\",\"lotus\",\n",
    "    \"ds\",\"mini\",\"smart\",\"saab\",\"chery\",\"nio\",\"xpeng\",\"lucid\",\"rivian\",\"tata\",\"mahindra\",\"proton\",\n",
    "    \"perodua\",\"holden\",\"daewoo\"\n",
    "]\n",
    "SHORT_WHITELIST = {\"uk\",\"us\",\"eu\",\"vw\",\"mg\",\"byd\",\"bmw\"}\n",
    "\n",
    "PRODUCT_PATTERNS = [\n",
    "    {\"label\":\"PRODUCT\",\"pattern\":[{\"LOWER\":\"model\"},{\"TEXT\":{\"REGEX\":\"[S3XY]\"}}]},  # Tesla Model S/3/X/Y\n",
    "    {\"label\":\"PRODUCT\",\"pattern\":[{\"LOWER\":{\"IN\":BRANDS}}, {\"TEXT\":{\"REGEX\":\"\\\\d{2,3}\"}}]},  # Porsche 911, 718\n",
    "    {\"label\":\"PRODUCT\",\"pattern\":[{\"LOWER\":{\"IN\":BRANDS}}, {\"TEXT\":{\"REGEX\":\"[A-Z]-?\\\\d{2,3}\"}}]},  # F-150, C-63\n",
    "    {\"label\":\"PRODUCT\",\"pattern\":[{\"LOWER\":{\"IN\":BRANDS}}, {\"LOWER\":{\"REGEX\":\"id\\\\.?|buzz\"}}, {\"TEXT\":{\"REGEX\":\"\\\\d(\\\\.\\\\d)?\"},\"OP\":\"?\"}]},  # VW ID.3/4/Buzz\n",
    "    {\"label\":\"PRODUCT\",\"pattern\":[{\"LOWER\":{\"IN\":BRANDS}}, {\"LOWER\":{\"REGEX\":\"gr|c-hr|yaris|corolla|supra|prius\"}}, {\"LOWER\":{\"REGEX\":\"cross|sport|prime|gr\"},\"OP\":\"?\"}]},  # Toyota\n",
    "    {\"label\":\"PRODUCT\",\"pattern\":[{\"LOWER\":{\"IN\":BRANDS}}, {\"TEXT\":{\"REGEX\":\"[MXi][A-Z]?$|[MXi]?\\\\d{1,3}\"}}]},  # BMW M3, X5, i5, iX\n",
    "    {\"label\":\"PRODUCT\",\"pattern\":[{\"LOWER\":{\"IN\":BRANDS}}, {\"TEXT\":{\"REGEX\":\"[A-Z]{2,3}\\\\d{0,3}\"}}]},  # EQS, GLC, etc.\n",
    "    {\"label\":\"PRODUCT\",\"pattern\":[{\"LOWER\":{\"IN\":BRANDS}}, {\"TEXT\":{\"REGEX\":\"[A-Za-z0-9-]{1,8}\"}}, {\"TEXT\":{\"REGEX\":\"gt|type|r|rs|gr\"},\"OP\":\"?\"}]},\n",
    "]\n",
    "\n",
    "# ---- Inject EntityRuler ----\n",
    "if \"entity_ruler\" not in nlp.pipe_names:\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", config={\"overwrite_ents\": False})\n",
    "else:\n",
    "    ruler = nlp.get_pipe(\"entity_ruler\")\n",
    "\n",
    "brand_patterns = [{\"label\":\"ORG\", \"pattern\": b} for b in BRANDS]\n",
    "ruler.add_patterns(brand_patterns + PRODUCT_PATTERNS)\n",
    "\n",
    "KEEP = {\"ORG\",\"GPE\",\"PRODUCT\",\"PERSON\"}\n",
    "\n",
    "def normalize_entity(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    low = s.lower()\n",
    "    if len(low) <= 2 and low not in SHORT_WHITELIST:\n",
    "        return \"\"\n",
    "    return low\n",
    "\n",
    "# ---- Batch extraction for speed ----\n",
    "ents_col = []\n",
    "with nlp.select_pipes(disable=[p for p in nlp.pipe_names if p not in {\"transformer\",\"entity_ruler\",\"ner\"}]):\n",
    "    for doc in nlp.pipe(df[\"text\"].tolist(), batch_size=64):\n",
    "        out = []\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in KEEP:\n",
    "                norm = normalize_entity(ent.text)\n",
    "                if norm:\n",
    "                    out.append((norm, ent.label_))\n",
    "        ents_col.append(out)\n",
    "\n",
    "df[\"named_entities\"] = ents_col\n",
    "\n",
    "# ---- Flatten + aggregate ----\n",
    "all_ents = [e for es in df[\"named_entities\"] for e in es]\n",
    "entity_df = pd.DataFrame(all_ents, columns=[\"entity\",\"label\"])\n",
    "entity_counts = entity_df.groupby([\"label\",\"entity\"]).size().reset_index(name=\"count\")\n",
    "top_entities = entity_counts.sort_values(\"count\", ascending=False).head(200)\n",
    "top_entities.to_sql(\"car_reviews_named_entities\", engine, index=False, if_exists=\"replace\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30247747-8298-406a-b5a0-dbbda3c8c3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72f3543b-95a1-4d3f-aef8-4db622e367d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated car_review.sentiment_score and car_review.sentiment_label without changing any column names.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# --- DB ---\n",
    "#engine = create_engine('postgresql://auto_intel:auto-intel@localhost/auto-intel')\n",
    "DB_URL = \"postgresql://auto_intel_user:G2ifL76ULDi9YOFJ4tHZ1ikUEORMx7Oe@dpg-d2i7ptvdiees73d1b4lg-a.oregon-postgres.render.com/auto_intel\"\n",
    "engine = create_engine(DB_URL, pool_pre_ping=True)\n",
    "\n",
    "# --- Load only what we need ---\n",
    "df = pd.read_sql(\"SELECT id, title, verdict FROM car_review;\", engine)\n",
    "df[[\"title\",\"verdict\"]] = df[[\"title\",\"verdict\"]].astype(str).fillna(\"\")\n",
    "\n",
    "# --- Domain phrase normalisation (multiword -> single token) ---\n",
    "# PHRASE_MAP = {\n",
    "#     r\"\\bbody roll\\b\": \"body_roll\",\n",
    "#     r\"\\btorque steer\\b\": \"torque_steer\",\n",
    "#     r\"\\broad noise\\b\": \"road_noise\",\n",
    "#     r\"\\bwind noise\\b\": \"wind_noise\",\n",
    "#     r\"\\bstopping distance\\b\": \"stopping_distance\",\n",
    "#     r\"\\bsoft touch\\b\": \"soft_touch\",\n",
    "#     r\"\\bwell put together\\b\": \"well_put_together\",\n",
    "#     r\"\\bon rails\\b\": \"on_rails\",\n",
    "#     r\"\\bdead steering\\b\": \"dead_steering\",\n",
    "#     r\"\\brange anxiety\\b\": \"range_anxiety\",\n",
    "# }\n",
    "\n",
    "# def normalise_phrases(t: str) -> str:\n",
    "#     out = t\n",
    "#     for pat, repl in PHRASE_MAP.items():\n",
    "#         out = re.sub(pat, repl, out, flags=re.IGNORECASE)\n",
    "#     return out\n",
    "\n",
    "# # --- VADER with automotive lexicon tweaks ---\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "# analyzer.lexicon.update({\n",
    "#     # negatives\n",
    "#     \"laggy\": -1.8, \"numb\": -1.6, \"floaty\": -1.4, \"spongy\": -1.6, \"crashy\": -1.9,\n",
    "#     \"boomy\": -1.3, \"buzzy\": -1.2, \"harsh\": -1.6, \"gutless\": -2.0, \"unrefined\": -1.7,\n",
    "#     \"body_roll\": -1.8, \"torque_steer\": -1.6, \"dead_steering\": -2.0, \"range_anxiety\": -2.0,\n",
    "#     # positives\n",
    "#     \"planted\": 1.8, \"grippy\": 1.7, \"refined\": 1.6, \"punchy\": 1.9, \"buttery\": 1.6,\n",
    "#     \"engaging\": 1.6, \"on_rails\": 2.0, \"well_put_together\": 1.7, \"soft_touch\": 1.3,\n",
    "# })\n",
    "\n",
    "PHRASE_MAP = {\n",
    "    # handling & dynamics\n",
    "    \"body roll\": \"body_roll\",\n",
    "    \"torque steer\": \"torque_steer\",\n",
    "    \"dead steering\": \"dead_steering\",\n",
    "    \"bump steer\": \"bump_steer\",\n",
    "    \"under steer\": \"understeer\",\n",
    "    \"over steer\": \"oversteer\",\n",
    "    \"brake fade\": \"brake_fade\",\n",
    "    \"wheel hop\": \"wheel_hop\",\n",
    "    \"tram lining\": \"tramlining\",\n",
    "    \"steering feel\": \"steering_feel\",\n",
    "    \"steering feedback\": \"steering_feedback\",\n",
    "\n",
    "    # NVH / build\n",
    "    \"road noise\": \"road_noise\",\n",
    "    \"wind noise\": \"wind_noise\",\n",
    "    \"tire noise\": \"tire_noise\",\n",
    "    \"cheap plastics\": \"cheap_plastics\",\n",
    "    \"hard plastics\": \"hard_plastics\",\n",
    "    \"panel gap\": \"panel_gap\",\n",
    "    \"fit and finish\": \"fit_and_finish\",\n",
    "    \"build quality\": \"build_quality\",\n",
    "    \"rear visibility\": \"rear_visibility\",\n",
    "\n",
    "    # powertrain\n",
    "    \"turbo lag\": \"turbo_lag\",\n",
    "    \"fuel economy\": \"fuel_economy\",\n",
    "    \"charging speed\": \"charging_speed\",\n",
    "\n",
    "    # good phrases (for balance)\n",
    "    \"on rails\": \"on_rails\",\n",
    "    \"well put together\": \"well_put_together\",\n",
    "    \"soft touch\": \"soft_touch\",\n",
    "    \"ride quality\": \"ride_quality\",\n",
    "}\n",
    "\n",
    "def normalize_phrases(text: str) -> str:\n",
    "    t = text.lower()\n",
    "    for k, v in PHRASE_MAP.items():\n",
    "        t = re.sub(rf\"\\b{re.escape(k)}\\b\", v, t)\n",
    "    # also collapse spaces->underscores for any phrase we already added manually\n",
    "    t = re.sub(r\"\\b(body)\\s+(roll)\\b\", r\"\\1_\\2\", t)\n",
    "    t = re.sub(r\"\\b(torque)\\s+(steer)\\b\", r\"\\1_\\2\", t)\n",
    "    t = re.sub(r\"\\b(dead)\\s+(steering)\\b\", r\"\\1_\\2\", t)\n",
    "    t = re.sub(r\"\\b(range)\\s+(anxiety)\\b\", r\"\\1_\\2\", t)\n",
    "    return t\n",
    "\n",
    "# --- 2) build a VADER analyzer with car-domain lexicon\n",
    "def build_auto_vader() -> SentimentIntensityAnalyzer:\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    analyzer.lexicon.update({\n",
    "        # negatives (dynamics / ride / steering)\n",
    "        \"laggy\": -1.8, \"sluggish\": -1.6, \"anemic\": -1.7, \"lethargic\": -1.5,\n",
    "        \"numb\": -1.6, \"vague\": -1.2, \"floaty\": -1.4, \"spongy\": -1.6,\n",
    "        \"crashy\": -1.9, \"choppy\": -1.5, \"jerky\": -1.6, \"lurchy\": -1.6,\n",
    "        \"understeer\": -1.5, \"oversteer\": -1.4, \"bump_steer\": -1.5,\n",
    "        \"body_roll\": -1.8, \"torque_steer\": -1.6, \"dead_steering\": -2.0,\n",
    "        \"brake_fade\": -2.1, \"grabby\": -1.2, \"wheel_hop\": -1.5,\n",
    "\n",
    "        # negatives (NVH / powertrain / build)\n",
    "        \"boomy\": -1.3, \"buzzy\": -1.2, \"droney\": -1.5, \"droning\": -1.5,\n",
    "        \"tinny\": -1.5, \"coarse\": -1.5, \"thrashy\": -1.6, \"harsh\": -1.6,\n",
    "        \"gutless\": -2.0, \"unrefined\": -1.7, \"nvh\": -1.7, \"rattly\": -1.8,\n",
    "        \"rattles\": -1.8, \"squeaks\": -1.5, \"creaks\": -1.6, \"flimsy\": -1.5,\n",
    "        \"plasticky\": -1.6, \"hard_plastics\": -1.4, \"cheap_plastics\": -1.8,\n",
    "        \"panel_gap\": -1.5, \"misaligned\": -1.3,\n",
    "        \"road_noise\": -1.5, \"wind_noise\": -1.4, \"tire_noise\": -1.3,\n",
    "        \"turbo_lag\": -1.7, \"range_anxiety\": -2.0,\n",
    "        \"cramped\": -1.6, \"claustrophobic\": -1.8, \"blind_spots\": -1.6,\n",
    "        \"rear_visibility\": -1.4,\n",
    "\n",
    "        # positives (for balance)\n",
    "        \"planted\": 1.8, \"grippy\": 1.7, \"refined\": 1.6, \"punchy\": 1.9,\n",
    "        \"buttery\": 1.6, \"engaging\": 1.6, \"on_rails\": 2.0,\n",
    "        \"well_put_together\": 1.7, \"soft_touch\": 1.3,\n",
    "        \"ride_quality\": 1.2, \"fit_and_finish\": 1.3, \"build_quality\": 1.1,\n",
    "        \"steering_feedback\": 1.0, \"steering_feel\": 0.8,\n",
    "    })\n",
    "    return analyzer\n",
    "\n",
    "analyzer = build_auto_vader()\n",
    "\n",
    "\n",
    "def score_text(title: str, verdict: str) -> float:\n",
    "    t = (title + \". \" + verdict).strip()\n",
    "    if not t:\n",
    "        return 0.0\n",
    "    t = normalise_phrases(t)\n",
    "    return analyzer.polarity_scores(t)[\"compound\"]\n",
    "\n",
    "def to_label(score: float, pos=0.05, neg=-0.05) -> str:\n",
    "    if score >= pos: return \"positive\"\n",
    "    if score <= neg: return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "# --- Compute (ONLY in memory; we will write back the two columns) ---\n",
    "df[\"sentiment_score\"] = [score_text(a, b) for a, b in zip(df[\"title\"], df[\"verdict\"])]\n",
    "df[\"sentiment_label\"] = df[\"sentiment_score\"].apply(to_label)\n",
    "\n",
    "# --- Stage results with EXACT column names ---\n",
    "staging = df[[\"id\",\"sentiment_score\",\"sentiment_label\"]].copy()\n",
    "staging.to_sql(\"car_review_sentiment_tmp\", engine, index=False, if_exists=\"replace\")\n",
    "\n",
    "# # --- Merge into main table WITHOUT renaming or replacing the table ---\n",
    "# with engine.begin() as con:\n",
    "#     con.execute(text(\"\"\"\n",
    "#         ALTER TABLE car_review \n",
    "#         ADD COLUMN IF NOT EXISTS sentiment_score DOUBLE PRECISION,\n",
    "#         ADD COLUMN IF NOT EXISTS sentiment_label TEXT;\n",
    "#         UPDATE car_review AS r\n",
    "#         SET sentiment_score = s.sentiment_score,\n",
    "#             sentiment_label = s.sentiment_label\n",
    "#         FROM car_review_sentiment_tmp AS s\n",
    "#         WHERE r.id = s.id;\n",
    "#         DROP TABLE car_review_sentiment_tmp;\n",
    "#     \"\"\"))\n",
    "\n",
    "print(\"Updated car_review.sentiment_score and car_review.sentiment_label without changing any column names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be97f3b2-52be-42c1-af38-8a280bc5dbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46384cb7-b469-418c-b50d-10200a4dcb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kb/rn4647j13zjfx809qkvrs8dc0000gn/T/ipykernel_13262/4203297208.py:20: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df[\"publication_date\"] = df[\"publication_date\"].dt.to_period(\"M\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "#engine = create_engine('postgresql://auto_intel:auto-intel@localhost/auto-intel')\n",
    "DB_URL = \"postgresql://auto_intel_user:G2ifL76ULDi9YOFJ4tHZ1ikUEORMx7Oe@dpg-d2i7ptvdiees73d1b4lg-a.oregon-postgres.render.com/auto_intel\"\n",
    "engine = create_engine(DB_URL, pool_pre_ping=True)\n",
    "\n",
    "def get_sentiment_trend(engine):\n",
    "    # Pull only needed cols; guard against nulls/bad dates\n",
    "    df = pd.read_sql(\n",
    "        \"SELECT publication_date, sentiment_score, sentiment_label FROM car_review WHERE publication_date IS NOT NULL\",\n",
    "        engine\n",
    "    )\n",
    "    # Parse to datetime (UTC); drop rows that still fail parsing\n",
    "    df[\"publication_date\"] = pd.to_datetime(df[\"publication_date\"], errors=\"coerce\", utc=True)\n",
    "    df = df.dropna(subset=[\"publication_date\"])\n",
    "\n",
    "    # Group by calendar month\n",
    "    df[\"publication_date\"] = df[\"publication_date\"].dt.to_period(\"M\")\n",
    "\n",
    "    trend_df = (\n",
    "        df.groupby(\"publication_date\")\n",
    "          .agg(\n",
    "              avg_sentiment=(\"sentiment_score\", \"mean\"),\n",
    "              positive=(\"sentiment_label\", lambda s: (s == \"positive\").sum()),\n",
    "              negative=(\"sentiment_label\", lambda s: (s == \"negative\").sum()),\n",
    "              neutral =(\"sentiment_label\", lambda s: (s == \"neutral\").sum()),\n",
    "          )\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    # Keep your column name 'publication_date' but as \"YYYY-MM\" strings\n",
    "    trend_df[\"publication_date\"] = trend_df[\"publication_date\"].astype(str)\n",
    "    # Cast counts to int\n",
    "    trend_df[[\"positive\",\"negative\",\"neutral\"]] = trend_df[[\"positive\",\"negative\",\"neutral\"]].astype(int)\n",
    "    # Sort by month string\n",
    "    trend_df = trend_df.sort_values(\"publication_date\").reset_index(drop=True)\n",
    "\n",
    "    return trend_df\n",
    "\n",
    "# Save to SQL (same table/column names you used)\n",
    "trend_df = get_sentiment_trend(engine)\n",
    "trend_df.to_sql(\"sentiment_trend_monthly\", con=engine, index=False, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cbcef6-5043-4e23-8691-29759c141a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f8a891c-dfec-4237-adc4-745337cc7887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kb/rn4647j13zjfx809qkvrs8dc0000gn/T/ipykernel_13262/3421616466.py:25: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df[\"publication_date\"] = df[\"publication_date\"].dt.to_period(\"M\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "#engine = create_engine('postgresql://auto_intel:auto-intel@localhost/auto-intel')\n",
    "DB_URL = \"postgresql://auto_intel_user:G2ifL76ULDi9YOFJ4tHZ1ikUEORMx7Oe@dpg-d2i7ptvdiees73d1b4lg-a.oregon-postgres.render.com/auto_intel\"\n",
    "engine = create_engine(DB_URL, pool_pre_ping=True)\n",
    "\n",
    "# # Market trends\n",
    "def get_market_trend(engine):\n",
    "    df = pd.read_sql(\"SELECT publication_date, price, rating FROM car_review\", engine)\n",
    "\n",
    "    # Parse dates safely; drop rows with invalid/missing dates\n",
    "    df[\"publication_date\"] = pd.to_datetime(df[\"publication_date\"], errors=\"coerce\", utc=True)\n",
    "    df = df.dropna(subset=[\"publication_date\"])\n",
    "\n",
    "    # Optional: align to London before month bucketing\n",
    "    # df[\"publication_date\"] = df[\"publication_date\"].dt.tz_convert(\"Europe/London\")\n",
    "\n",
    "    # Ensure numeric for aggregations (invalid -> NaN, means will skip NaN)\n",
    "    df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "    df[\"rating\"] = pd.to_numeric(df[\"rating\"], errors=\"coerce\")\n",
    "\n",
    "    # Group by calendar month\n",
    "    df[\"publication_date\"] = df[\"publication_date\"].dt.to_period(\"M\")\n",
    "\n",
    "    market_df = (\n",
    "        df.groupby(\"publication_date\")\n",
    "          .agg(\n",
    "              avg_price = (\"price\", \"mean\"),\n",
    "              avg_rating = (\"rating\", \"mean\"),\n",
    "              article_count = (\"publication_date\", \"size\")  # counts all rows\n",
    "          )\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    # Keep your column name 'publication_date' but as YYYY-MM strings\n",
    "    market_df[\"publication_date\"] = market_df[\"publication_date\"].astype(str)\n",
    "\n",
    "    # Sort by month\n",
    "    market_df = market_df.sort_values(\"publication_date\").reset_index(drop=True)\n",
    "\n",
    "    return market_df\n",
    "\n",
    "# Save the table\n",
    "market_df = get_market_trend(engine)\n",
    "market_df.to_sql(\"market_trend_monthly\", con=engine, index=False, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac692176-703a-477b-b263-67529fe44c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "946a5028-3620-4472-847c-200c2f9c62d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Supervised Sentiment (TF-IDF + LR) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.500     0.048     0.087        21\n",
      "     neutral      0.784     0.996     0.877       233\n",
      "    positive      0.980     0.761     0.857       197\n",
      "\n",
      "    accuracy                          0.849       451\n",
      "   macro avg      0.755     0.602     0.607       451\n",
      "weighted avg      0.856     0.849     0.832       451\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  1  17   3]\n",
      " [  1 232   0]\n",
      " [  0  47 150]]\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Optional: Supervised Sentiment (Much Stronger) --------------------\n",
    "# Uses TF-IDF + Logistic Regression with class_weight='balanced' and n-grams\n",
    "# Comment this block out if you only want VADER.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Use your existing sentiment_label from annotation if available.\n",
    "# If you only have VADER, you can keep this section off or treat VADER as pseudo-labels (not recommended).\n",
    "labels = df['sentiment_label']  # replace with gold labels if you have them\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['verdict'], labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        analyzer=\"word\",\n",
    "        ngram_range=(1,2),         # capture \"not good\", \"very bad\"\n",
    "        min_df=3,\n",
    "        max_df=0.8,\n",
    "        strip_accents=\"unicode\",\n",
    "        sublinear_tf=True\n",
    "    )),\n",
    "    (\"logreg\", LogisticRegression(\n",
    "        max_iter=300,\n",
    "        class_weight=\"balanced\",   # handle imbalance\n",
    "        n_jobs=None,\n",
    "        C=2.0                      # a touch less regularisation than default; tune if needed\n",
    "    ))\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Supervised Sentiment (TF-IDF + LR) ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edefb61-28fe-4f2e-ac8a-34a008b7e217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f3bd5-eea5-4b18-8a3a-34da9551c343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd553bb6-dc87-4e95-8d82-8664b43886c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb7e098-e222-4b3e-b960-d0a7fb902a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f87233-2ae9-404d-abe6-e213d27a3ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516e94c-024f-4174-9f0e-786f9f954d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ebf3ca-f3f2-4565-8443-c389988e9e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb70e3c-001f-4875-b88f-08ebebbedd31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc67c94-6fa5-4507-8a18-770b8bcc55df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650adcbe-b370-4fb6-a63d-219ed51718e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b6271a-0bc6-4a19-9321-68fbb405ee8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731cab1d-e86a-43cf-a670-b41b07ad95a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ad7cf1-f739-439d-8ab3-ad381b6804fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Proxy metrics (mentions & distincts per label) ----\n",
    "proxy = entity_counts.groupby(\"label\").agg(\n",
    "    mentions=(\"count\",\"sum\"),\n",
    "    distinct_entities=(\"entity\",\"nunique\")\n",
    ").reset_index()\n",
    "proxy.to_sql(\"car_reviews_ner_proxy_metrics\", engine, index=False, if_exists=\"replace\")\n",
    "\n",
    "print(\"Saved: car_reviews_named_entities, car_reviews_ner_proxy_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7244eee-6636-431c-a908-30b5f7297085",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_trf'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_trf\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# or en_core_web_md/sm\u001b[39;00m\n\u001b[1;32m      7\u001b[0m CSV_GOLD \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner_eval_gold_reviews.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(MODEL)\n\u001b[1;32m     10\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_engine(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostgresql://auto_intel:auto-intel@localhost/auto-intel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_examples\u001b[39m(csv_path):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/__init__.py:52\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     29\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     36\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m     53\u001b[0m         name,\n\u001b[1;32m     54\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m     55\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[1;32m     56\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[1;32m     57\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[1;32m     58\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     59\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/util.py:484\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_trf'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# ner_eval_reviews_to_db.py\n",
    "import json, pandas as pd, spacy, datetime as dt\n",
    "from spacy.training import Example\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "MODEL = \"en_core_web_trf\"  # or en_core_web_md/sm\n",
    "CSV_GOLD = \"ner_eval_gold_reviews.csv\"\n",
    "\n",
    "nlp = spacy.load(MODEL)\n",
    "engine = create_engine('postgresql://auto_intel:auto-intel@localhost/auto-intel')\n",
    "\n",
    "def load_examples(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    examples = []\n",
    "    for _, r in df.iterrows():\n",
    "        text = str(r[\"text\"])\n",
    "        spans = json.loads(r[\"spans\"]) if isinstance(r[\"spans\"], str) else r[\"spans\"]\n",
    "        gold = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for s in spans:\n",
    "            span = gold.char_span(int(s[\"start\"]), int(s[\"end\"]), label=str(s[\"label\"]), alignment_mode=\"contract\")\n",
    "            if span: ents.append(span)\n",
    "        gold.ents = ents\n",
    "        pred = nlp(text)\n",
    "        examples.append(Example(pred, gold))\n",
    "    return examples\n",
    "\n",
    "def score_ner(nlp, examples):\n",
    "    from spacy.scorer import Scorer\n",
    "    sc = Scorer()\n",
    "    for eg in examples: sc.score(eg)\n",
    "    return sc.score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    exs = load_examples(CSV_GOLD)\n",
    "    s = score_ner(nlp, exs)\n",
    "\n",
    "    overall = pd.DataFrame([{\n",
    "        \"model\": MODEL,\n",
    "        \"run_at\": dt.datetime.utcnow(),\n",
    "        \"ents_p\": s.get(\"ents_p\", 0.0),\n",
    "        \"ents_r\": s.get(\"ents_r\", 0.0),\n",
    "        \"ents_f\": s.get(\"ents_f\", 0.0),\n",
    "        \"n_examples\": len(exs),\n",
    "        \"domain\": \"car_review\"\n",
    "    }])\n",
    "    per = []\n",
    "    for lbl, d in s.get(\"ents_per_type\", {}).items():\n",
    "        per.append({\n",
    "            \"model\": MODEL, \"run_at\": dt.datetime.utcnow(),\n",
    "            \"label\": lbl, \"p\": d.get(\"p\",0.0), \"r\": d.get(\"r\",0.0), \"f1\": d.get(\"f\",0.0),\n",
    "            \"domain\": \"car_review\"\n",
    "        })\n",
    "    per_df = pd.DataFrame(per) if per else pd.DataFrame(columns=[\"model\",\"run_at\",\"label\",\"p\",\"r\",\"f1\",\"domain\"])\n",
    "\n",
    "    overall.to_sql(\"ner_eval_overall\", engine, if_exists=\"append\", index=False)\n",
    "    per_df.to_sql(\"ner_eval_per_label\", engine, if_exists=\"append\", index=False)\n",
    "    print(\"Saved: ner_eval_overall, ner_eval_per_label (domain=car_review)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e5909-a64e-49c2-8d64-70ed6f989077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201fb5fa-077a-4d6e-ba7e-4e3115b47bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c497a7d-3f3b-40f6-9358-5f49754bbb9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b105f61e-bf5c-4351-a4ea-08452ebc0a12",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Select CSV or DocBin loading in the __main__ block before running.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Select CSV or DocBin loading in the __main__ block before running.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# ner_evaluate.py\n",
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_gold_from_csv(csv_path, nlp):\n",
    "    \"\"\"CSV columns: id,text,spans (JSON list of {start,end,label})\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    examples = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = str(row[\"text\"])\n",
    "        spans_json = row[\"spans\"]\n",
    "        spans = json.loads(spans_json) if isinstance(spans_json, str) else spans_json\n",
    "        # Build gold doc\n",
    "        gold_doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for s in spans:\n",
    "            span = gold_doc.char_span(int(s[\"start\"]), int(s[\"end\"]), label=str(s[\"label\"]), alignment_mode=\"contract\")\n",
    "            if span is not None:\n",
    "                ents.append(span)\n",
    "        gold_doc.ents = ents\n",
    "        # Build predicted doc\n",
    "        pred_doc = nlp(text)\n",
    "        examples.append(Example(pred_doc, gold_doc))\n",
    "    return examples\n",
    "\n",
    "def load_gold_from_docbin(docbin_path, nlp):\n",
    "    \"\"\"DocBin should contain GOLD docs with .ents set.\"\"\"\n",
    "    db = DocBin().from_disk(docbin_path)\n",
    "    gold_docs = list(db.get_docs(nlp.vocab))\n",
    "    examples = [Example(nlp(d.text), d) for d in gold_docs]\n",
    "    return examples\n",
    "\n",
    "def score_examples(nlp, examples):\n",
    "    scorer = Scorer()\n",
    "    for eg in examples:\n",
    "        scorer.score(eg)\n",
    "    scores = scorer.score\n",
    "    return scores\n",
    "\n",
    "def pretty_print_ner_scores(scores):\n",
    "    ents = scores.get(\"ents_f\", 0.0)\n",
    "    p = scores.get(\"ents_p\", 0.0)\n",
    "    r = scores.get(\"ents_r\", 0.0)\n",
    "    print(\"\\n=== NER Overall ===\")\n",
    "    print(f\"Precision: {p:.3f}  Recall: {r:.3f}  F1: {ents:.3f}\")\n",
    "\n",
    "    print(\"\\n--- Per-label ---\")\n",
    "    per_type = scores.get(\"ents_per_type\", {})\n",
    "    for label, s in per_type.items():\n",
    "        print(f\"{label:>10s}  P: {s.get('p',0):.3f}  R: {s.get('r',0):.3f}  F1: {s.get('f',0):.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose model to evaluate\n",
    "    model_name = \"en_core_web_md\"   # or \"en_core_web_trf\"\n",
    "    nlp = spacy.load(model_name)\n",
    "\n",
    "    # EITHER: evaluate from CSV\n",
    "    # csv_path = \"ner_eval.csv\"\n",
    "    # examples = load_gold_from_csv(csv_path, nlp)\n",
    "\n",
    "    # OR: evaluate from DocBin\n",
    "    # docbin_path = \"ner_eval.spacy\"\n",
    "    # examples = load_gold_from_docbin(docbin_path, nlp)\n",
    "\n",
    "    # ---- SELECT ONE SOURCE ----\n",
    "    # Uncomment exactly one of the above two blocks and comment the other.\n",
    "    raise SystemExit(\"Select CSV or DocBin loading in the __main__ block before running.\")\n",
    "\n",
    "    scores = score_examples(nlp, examples)\n",
    "    pretty_print_ner_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb63e2f-dd53-46d8-ba95-b1d33c6d745a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
